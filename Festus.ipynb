{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\folan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_compound: Correlation = None, p-value = None\n",
      "average_positive: Correlation = None, p-value = None\n",
      "average_negative: Correlation = None, p-value = None\n",
      "average_compound_lagged: Correlation = None, p-value = None\n",
      "average_positive_lagged: Correlation = None, p-value = None\n",
      "average_negative_lagged: Correlation = None, p-value = None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\folan\\AppData\\Local\\Temp\\ipykernel_41896\\3490489758.py:164: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  stock_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# import re\n",
    "# from scipy.stats import spearmanr\n",
    "# import nltk\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# # ------------------------\n",
    "# # 1. Preprocessing Tweets\n",
    "# # ------------------------\n",
    "# data = pd.read_csv('Tesla.csv')\n",
    "\n",
    "# cleaned_data = pd.DataFrame()\n",
    "# cleaned_data['original_tweet'] = data['tweet']\n",
    "# cleaned_data['cleaned_tweet'] = \"\"\n",
    "# cleaned_data['compound_score'] = 0.0\n",
    "# cleaned_data['positive_score'] = 0.0\n",
    "# cleaned_data['negative_score'] = 0.0\n",
    "# cleaned_data['neutral_score'] = 0.0\n",
    "\n",
    "# # Preprocess tweets\n",
    "# def preprocess_tweet(tweet):\n",
    "#     tweet = str(tweet)\n",
    "#     tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)  # Remove URLs\n",
    "#     tweet = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
    "#     tweet = re.sub(r'#', '', tweet)  # Remove hashtags\n",
    "#     tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove special characters\n",
    "#     tweet = re.sub(r'\\d+', '', tweet)  # Remove numbers\n",
    "#     tweet = tweet.strip()  # Remove leading/trailing whitespace\n",
    "#     return tweet\n",
    "\n",
    "# cleaned_data['cleaned_tweet'] = data['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# # Sentiment analysis\n",
    "# sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "# for i in range(len(cleaned_data)):\n",
    "#     curr_tweet = cleaned_data[\"cleaned_tweet\"][i]\n",
    "#     curr_score = sentiment_analyzer.polarity_scores(curr_tweet)\n",
    "#     cleaned_data.loc[i, \"compound_score\"] = curr_score[\"compound\"]\n",
    "#     cleaned_data.loc[i, \"positive_score\"] = curr_score[\"pos\"]\n",
    "#     cleaned_data.loc[i, \"negative_score\"] = curr_score[\"neg\"]\n",
    "#     cleaned_data.loc[i, \"neutral_score\"] = curr_score[\"neu\"]\n",
    "\n",
    "# # ------------------------\n",
    "# # 2. Aggregate Sentiment Scores\n",
    "# # ------------------------\n",
    "# daily_sentiment = pd.DataFrame({\n",
    "#     'date': [data['date'].iloc[0]],  # Assuming all tweets are from the same day\n",
    "#     'average_compound': [cleaned_data['compound_score'].mean()],\n",
    "#     'average_positive': [cleaned_data['positive_score'].mean()],\n",
    "#     'average_negative': [cleaned_data['negative_score'].mean()],\n",
    "#     'average_neutral': [cleaned_data['neutral_score'].mean()],\n",
    "#     'total_tweets': [len(cleaned_data)]\n",
    "# })\n",
    "\n",
    "# # ------------------------\n",
    "# # 3. Load and Prepare Stock Data\n",
    "# # ------------------------\n",
    "# stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "# stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "# stock_data['Return'] = stock_data['Close'].pct_change()\n",
    "\n",
    "# # ------------------------\n",
    "# # 4. Align Data\n",
    "# # ------------------------\n",
    "# daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
    "# merged_data = pd.merge(daily_sentiment, stock_data, left_on='date', right_on='Date', how='inner')\n",
    "\n",
    "# # ------------------------\n",
    "# # 5. Experiment with Correlations\n",
    "# # ------------------------\n",
    "# # Same-day correlations\n",
    "# correlations = {}\n",
    "# for sentiment_metric in ['average_compound', 'average_positive', 'average_negative']:\n",
    "#     correlation, p_value = spearmanr(merged_data[sentiment_metric], merged_data['Return'])\n",
    "#     correlations[sentiment_metric] = {'correlation': correlation, 'p_value': p_value}\n",
    "\n",
    "# # Lagged correlations\n",
    "# merged_data['Lagged_Return'] = merged_data['Return'].shift(-1)\n",
    "# for sentiment_metric in ['average_compound', 'average_positive', 'average_negative']:\n",
    "#     correlation, p_value = spearmanr(merged_data[sentiment_metric], merged_data['Lagged_Return'])\n",
    "#     correlations[f'{sentiment_metric}_lagged'] = {'correlation': correlation, 'p_value': p_value}\n",
    "\n",
    "# # Display results\n",
    "# for key, value in correlations.items():\n",
    "#     print(f\"{key}: Correlation = {value['correlation']}, p-value = {value['p_value']}\")\n",
    "\n",
    "#  ////////////////////////////////////////////////////////////////////////\n",
    "#  ////////////////////////////////////////////////////////////////////////\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# ------------------------\n",
    "# 1. Preprocessing Tweets\n",
    "# ------------------------\n",
    "data = pd.read_csv('Tesla.csv')\n",
    "\n",
    "cleaned_data = pd.DataFrame()\n",
    "cleaned_data['original_tweet'] = data['tweet']\n",
    "cleaned_data['cleaned_tweet'] = \"\"\n",
    "cleaned_data['compound_score'] = 0.0\n",
    "cleaned_data['positive_score'] = 0.0\n",
    "cleaned_data['negative_score'] = 0.0\n",
    "cleaned_data['neutral_score'] = 0.0\n",
    "\n",
    "# Preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#', '', tweet)  # Remove hashtags\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove special characters\n",
    "    tweet = re.sub(r'\\d+', '', tweet)  # Remove numbers\n",
    "    tweet = tweet.strip()  # Remove leading/trailing whitespace\n",
    "    return tweet\n",
    "\n",
    "cleaned_data['cleaned_tweet'] = data['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Sentiment analysis\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "for i in range(len(cleaned_data)):\n",
    "    curr_tweet = cleaned_data[\"cleaned_tweet\"][i]\n",
    "    curr_score = sentiment_analyzer.polarity_scores(curr_tweet)\n",
    "    cleaned_data.loc[i, \"compound_score\"] = curr_score[\"compound\"]\n",
    "    cleaned_data.loc[i, \"positive_score\"] = curr_score[\"pos\"]\n",
    "    cleaned_data.loc[i, \"negative_score\"] = curr_score[\"neg\"]\n",
    "    cleaned_data.loc[i, \"neutral_score\"] = curr_score[\"neu\"]\n",
    "\n",
    "# Ensure sentiment scores are numeric\n",
    "for column in ['compound_score', 'positive_score', 'negative_score', 'neutral_score']:\n",
    "    cleaned_data[column] = pd.to_numeric(cleaned_data[column], errors='coerce')\n",
    "cleaned_data.fillna(0, inplace=True)  # Replace any NaN with 0\n",
    "\n",
    "# ------------------------\n",
    "# 2. Aggregate Sentiment Scores\n",
    "# ------------------------\n",
    "# Convert date to datetime and extract only the date\n",
    "data['date'] = pd.to_datetime(data['date']).dt.date\n",
    "\n",
    "daily_sentiment = pd.DataFrame({\n",
    "    'date': [data['date'].iloc[0]],  # Assuming all tweets are from the same day\n",
    "    'average_compound': [cleaned_data['compound_score'].mean()],\n",
    "    'average_positive': [cleaned_data['positive_score'].mean()],\n",
    "    'average_negative': [cleaned_data['negative_score'].mean()],\n",
    "    'average_neutral': [cleaned_data['neutral_score'].mean()],\n",
    "    'total_tweets': [len(cleaned_data)]\n",
    "})\n",
    "\n",
    "# ------------------------\n",
    "# 3. Load and Prepare Stock Data\n",
    "# ------------------------\n",
    "stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'])\n",
    "\n",
    "# Ensure 'Close' column is numeric and compute returns\n",
    "stock_data['Close'] = pd.to_numeric(stock_data['Close'], errors='coerce')\n",
    "stock_data.fillna(method='ffill', inplace=True)  # Forward fill missing values\n",
    "stock_data['Return'] = stock_data['Close'].pct_change()\n",
    "\n",
    "# ------------------------\n",
    "# 4. Align Data\n",
    "# ------------------------\n",
    "daily_sentiment['date'] = pd.to_datetime(daily_sentiment['date'])\n",
    "merged_data = pd.merge(daily_sentiment, stock_data, left_on='date', right_on='Date', how='inner')\n",
    "\n",
    "# ------------------------\n",
    "# 5. Experiment with Correlations\n",
    "# ------------------------\n",
    "correlations = {}\n",
    "\n",
    "# Same-day correlations\n",
    "for sentiment_metric in ['average_compound', 'average_positive', 'average_negative']:\n",
    "    if len(merged_data) > 1:  # Ensure enough data points for correlation\n",
    "        correlation, p_value = spearmanr(merged_data[sentiment_metric], merged_data['Return'])\n",
    "        correlations[sentiment_metric] = {'correlation': correlation, 'p_value': p_value}\n",
    "    else:\n",
    "        correlations[sentiment_metric] = {'correlation': None, 'p_value': None}\n",
    "\n",
    "# Lagged correlations\n",
    "merged_data['Lagged_Return'] = merged_data['Return'].shift(-1)\n",
    "for sentiment_metric in ['average_compound', 'average_positive', 'average_negative']:\n",
    "    if len(merged_data) > 1:  # Ensure enough data points for correlation\n",
    "        correlation, p_value = spearmanr(merged_data[sentiment_metric], merged_data['Lagged_Return'])\n",
    "        correlations[f'{sentiment_metric}_lagged'] = {'correlation': correlation, 'p_value': p_value}\n",
    "    else:\n",
    "        correlations[f'{sentiment_metric}_lagged'] = {'correlation': None, 'p_value': None}\n",
    "\n",
    "# Display results\n",
    "for key, value in correlations.items():\n",
    "    print(f\"{key}: Correlation = {value['correlation']}, p-value = {value['p_value']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
