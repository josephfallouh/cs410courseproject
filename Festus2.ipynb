{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\folan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla.csv loaded successfully.\n",
      "\n",
      "Sample Tweet Data:\n",
      "   Unnamed: 0                   id      conversation_id    created_at  \\\n",
      "0           0  1546541426317590528  1545826164564000768  1.657559e+12   \n",
      "1           1  1546541415857102850  1545826164564000768  1.657559e+12   \n",
      "2           2  1546541411897581568  1546541411897581568  1.657559e+12   \n",
      "3           3  1546541379110805508  1546340000500813824  1.657559e+12   \n",
      "4           4  1546541363470028800  1546541363470028800  1.657559e+12   \n",
      "\n",
      "                  date  timezone place  \\\n",
      "0  2022-07-11 17:06:24         0   NaN   \n",
      "1  2022-07-11 17:06:21         0   NaN   \n",
      "2  2022-07-11 17:06:20         0   NaN   \n",
      "3  2022-07-11 17:06:12         0   NaN   \n",
      "4  2022-07-11 17:06:09         0   NaN   \n",
      "\n",
      "                                               tweet language  \\\n",
      "0  @GailAlfarATX @elonmusk @Tesla @teslacn @Tesla...       en   \n",
      "1  @elonmusk @GailAlfarATX @Tesla @teslacn @Tesla...       en   \n",
      "2  @elonmusk #Think about buying a country , #Mex...       en   \n",
      "3  @get_innocuous Actual receipts, and yet you ha...       en   \n",
      "4  Tesla wall battery for the save! Power went ou...       en   \n",
      "\n",
      "                              hashtags  ... geo  source  user_rt_id user_rt  \\\n",
      "0                                   []  ... NaN     NaN         NaN     NaN   \n",
      "1                                   []  ... NaN     NaN         NaN     NaN   \n",
      "2  ['think', 'mexico', 'constitution']  ... NaN     NaN         NaN     NaN   \n",
      "3                                   []  ... NaN     NaN         NaN     NaN   \n",
      "4                                   []  ... NaN     NaN         NaN     NaN   \n",
      "\n",
      "  retweet_id                                           reply_to  retweet_date  \\\n",
      "0        NaN  [{'screen_name': 'GailAlfarATX', 'name': 'Gail...           NaN   \n",
      "1        NaN  [{'screen_name': 'elonmusk', 'name': 'Elon Mus...           NaN   \n",
      "2        NaN                                                 []           NaN   \n",
      "3        NaN  [{'screen_name': 'get_innocuous', 'name': 'tra...           NaN   \n",
      "4        NaN                                                 []           NaN   \n",
      "\n",
      "  translate trans_src trans_dest  \n",
      "0       NaN       NaN        NaN  \n",
      "1       NaN       NaN        NaN  \n",
      "2       NaN       NaN        NaN  \n",
      "3       NaN       NaN        NaN  \n",
      "4       NaN       NaN        NaN  \n",
      "\n",
      "[5 rows x 39 columns]\n",
      "\n",
      "Sentiment Category Counts:\n",
      "sentiment_category\n",
      "Neutral     4234\n",
      "Positive    3436\n",
      "Negative    2346\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sentiment distribution pie chart saved as 'sentiment_distribution_pie.png'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\folan\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compound score distribution histogram saved as 'compound_score_distribution.png'.\n",
      "\n",
      "Top 5 Positive Tweets:\n",
      "                                                                                                                                                                                                                                                                                                       tweet  compound_score\n",
      "                            @quineatal A great portion of wealthy people can trace their success to luck but the greatest part succeeds through hard work. Life is a choice and I made mine which is hard work because Newton, Einstein, and Tesla are my heroes. Sometimes, luck= opportunity + preparation          0.9829\n",
      "A mind of Einstein and Tesla , a pure rising talent in India , a pure talent house of a passionate actor and most importantly a passionate human being whose heart is filled with love , affection and kindness. This is our Sushant Singh Rajput  IntellectualPowerhouse Sushant‚ô•Ô∏è  https://t.co/oJ9fd19j8A          0.9766\n",
      "                         A mind of Einstein and Tesla , a pure rising talent in India , a pure talent house of a passionate actor and most importantly a passionate human being whose heart is filled with love , affection and kindness. This is our Sushant Singh Rajput  IntellectualPowerhouse Sushant‚ô•Ô∏è          0.9766\n",
      "                        @PPathole @elonmusk @Tesla Please dont forget he was the kind to give for free just to make humanity better... Associating his name with a rich company is a mistake to me. I respect him dearly.. His legacy cannot be counted with money.. His heart and mind were both admirable.          0.9728\n",
      "                 @mysticl I've loved Honda's since I was like 5, my family had one and I was in love with it. I developed love for Tesla's when I got older and more informed about clean energy and sustainable solutions. I have not made any comparisons whatsoever just a childhood desire that's all ‚ù§Ô∏è          0.9728\n",
      "\n",
      "Top 5 Negative Tweets:\n",
      "                                                                                                                                                                                                                                                                                      tweet  compound_score\n",
      "                                       Elon musk can taste my fat toes, he is a bitch and I hate him, also fuck tesla. Ever wonder why the police, firemen, ambulance drivers never drive electric cars? The UK tried electric police cars and they failed miserably. üñïüñïüñï SUCK MY DICK ELON         -0.9778\n",
      "                                         @JoeMagnus74 @Forbes He was for sure wrong on Solar city. And wrong again with PayPal. Wrong with Tesla. Wrong with space x. He is wrong for trying to buy Twitter. He is wrong for building the gigs factories. He is wrong‚Ä¶ maybe you‚Äôre wrong??         -0.9702\n",
      "@elonmusk You really need to stop making a fool of yourself with the Twitter disaster, and try to focus on helping Tesla recover from your self inflicted damage.  Pay the penalty to end the debacle of putting yourself up as the poster boy for White Supremacists on Twitter.  #Fascist         -0.9610\n",
      "                                     Elon Musk now faces serious liability associated with Tesla, its autopilot operating system, and a bevy of fatal accidents, one of them being an odd high-speed crash that was tragically fatal for an 18-year-old teen boy.   https://t.co/GGVy1QZ3hm         -0.9590\n",
      "                                             @masegoslin @Tesla @elonmusk @LucidMotors @Tesla2Lucid Nothing is forever best but it is a choice of people. Customers buy Ford's Mustang or F-150 that doesn't mean they are the best of best and others are the worst. It's a simple choice.         -0.9580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\folan\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "c:\\Users\\folan\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourly sentiment line plot saved as 'hourly_sentiment.png'.\n",
      "Word cloud 'positive_wordcloud.png' generated and saved.\n",
      "Word cloud 'negative_wordcloud.png' generated and saved.\n",
      "\n",
      "Topics Identified via LDA:\n",
      "Topic #1: plaid, auto, new, nikola, di, nie, amp, speed, model, tesla\n",
      "Topic #2: da, ceo, el, la, en, que, elon, twitter, musk, tesla\n",
      "Topic #3: das, el, es, der, und, en, die, que, la, tesla\n",
      "Topic #4: car, dont, musk, buy, like, elon, stock, just, twitter, tesla\n",
      "Topic #5: teslas, model, children, people, future, car, new, electric, ev, tesla\n",
      "\n",
      "TSLA_stock_data.csv loaded successfully.\n",
      "\n",
      "Sample Stock Data:\n",
      "         Date Adj Close   Close    High     Low    Open     Volume\n",
      "0         NaN      TSLA    TSLA    TSLA    TSLA    TSLA       TSLA\n",
      "1  2022-07-05    233.07  233.07  233.15  216.17   223.0   84581100\n",
      "2  2022-07-06    231.73  231.73  234.56  227.19  230.78   71853600\n",
      "3  2022-07-07    244.54  244.54  245.36  232.21  233.92   81930600\n",
      "4  2022-07-08    250.76  250.76  254.98  241.16  242.33  101854200\n",
      "\n",
      "Processed Stock Data with Returns:\n",
      "        Date Adj Close   Close    High     Low    Open    Volume    Return\n",
      "5 2022-07-11    234.34  234.34  253.06  233.63   252.1  99241200 -0.065481\n",
      "6 2022-07-12    233.07  233.07  239.77  228.37  236.85  87930900 -0.005419\n",
      "7 2022-07-13    237.04  237.04  242.06  225.03   225.5  97954500  0.017034\n",
      "8 2022-07-14    238.31  238.31  238.65  229.33   234.9  78557400  0.005358\n",
      "9 2022-07-15    240.07  240.07  243.62  236.89   240.0  69683100  0.007385\n",
      "\n",
      "Note: Correlation analysis between sentiment and stock performance is not feasible with only one day of sentiment data.\n",
      "For correlation, multiple days of sentiment and corresponding stock returns are required.\n",
      "However, you can still analyze and interpret the sentiment distribution and content for the single day.\n",
      "\n",
      "Analysis Complete.\n",
      "Generated the following visualizations:\n",
      "- 'sentiment_distribution_pie.png'\n",
      "- 'compound_score_distribution.png'\n",
      "- 'hourly_sentiment.png'\n",
      "- 'positive_wordcloud.png'\n",
      "- 'negative_wordcloud.png'\n",
      "\n",
      "Top positive and negative tweets have been printed to the console.\n",
      "Identified topics via LDA have been printed to the console.\n",
      "Sample and processed stock data have been printed to the console.\n"
     ]
    }
   ],
   "source": [
    "# # Festus.ipynb\n",
    "\n",
    "# # =========================================\n",
    "# # 0. Import Necessary Libraries\n",
    "# # =========================================\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import re\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import nltk\n",
    "# from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# from wordcloud import WordCloud\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# # Set plot styles for better aesthetics\n",
    "# sns.set(style='whitegrid')\n",
    "# plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# # Download the VADER lexicon for sentiment analysis\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# # =========================================\n",
    "# # 1. Load and Preprocess Tweet Data\n",
    "# # =========================================\n",
    "\n",
    "# # Load tweet data from 'Tesla.csv'\n",
    "# try:\n",
    "#     data = pd.read_csv('Tesla.csv')\n",
    "#     print(\"Tesla.csv loaded successfully.\")\n",
    "# except FileNotFoundError:\n",
    "#     raise FileNotFoundError(\"Tesla.csv file not found in the current directory.\")\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# print(\"\\nSample Tweet Data:\")\n",
    "# print(data.head())\n",
    "\n",
    "# # Ensure necessary columns are present\n",
    "# required_columns = ['tweet', 'date']\n",
    "# missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "# if missing_columns:\n",
    "#     raise ValueError(f\"Missing columns in Tesla.csv: {missing_columns}\")\n",
    "\n",
    "# # Convert 'date' column to datetime\n",
    "# data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "\n",
    "# # Check for 'created_at' column for temporal analysis\n",
    "# has_created_at = 'created_at' in data.columns\n",
    "\n",
    "# if has_created_at:\n",
    "#     data['created_at'] = pd.to_datetime(data['created_at'], errors='coerce')\n",
    "\n",
    "# # Preprocessing function for tweets\n",
    "# def preprocess_tweet(tweet):\n",
    "#     tweet = str(tweet)\n",
    "#     tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)      # Remove URLs\n",
    "#     tweet = re.sub(r'@\\w+', '', tweet)                         # Remove mentions\n",
    "#     tweet = re.sub(r'#', '', tweet)                            # Remove hash symbol\n",
    "#     tweet = re.sub(r'[^\\w\\s]', '', tweet)                      # Remove special characters/punctuation\n",
    "#     tweet = re.sub(r'\\d+', '', tweet)                          # Remove numbers\n",
    "#     tweet = tweet.strip()                                      # Remove leading/trailing whitespace\n",
    "#     return tweet\n",
    "\n",
    "# # Apply preprocessing to tweets\n",
    "# data['cleaned_tweet'] = data['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# # =========================================\n",
    "# # 2. Sentiment Analysis with VADER\n",
    "# # =========================================\n",
    "\n",
    "# # Initialize VADER sentiment analyzer\n",
    "# sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# # Calculate sentiment scores for each tweet\n",
    "# sentiment_scores = data['cleaned_tweet'].apply(sentiment_analyzer.polarity_scores)\n",
    "\n",
    "# # Extract individual sentiment scores into separate columns\n",
    "# data['compound_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "# data['positive_score'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "# data['negative_score'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "# data['neutral_score'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "\n",
    "# # Categorize tweets based on compound scores\n",
    "# def categorize_sentiment(score):\n",
    "#     if score >= 0.05:\n",
    "#         return 'Positive'\n",
    "#     elif score <= -0.05:\n",
    "#         return 'Negative'\n",
    "#     else:\n",
    "#         return 'Neutral'\n",
    "\n",
    "# data['sentiment_category'] = data['compound_score'].apply(categorize_sentiment)\n",
    "\n",
    "# # Display sentiment distribution\n",
    "# print(\"\\nSentiment Category Counts:\")\n",
    "# print(data['sentiment_category'].value_counts())\n",
    "\n",
    "# # =========================================\n",
    "# # 3. Descriptive Sentiment Analysis\n",
    "# # =========================================\n",
    "\n",
    "# # Overall sentiment distribution\n",
    "# sentiment_counts = data['sentiment_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "# # Pie Chart of Sentiment Distribution\n",
    "# plt.figure(figsize=(8,6))\n",
    "# plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
    "#         colors=['#66b3ff','#ff9999','#99ff99'], startangle=140)\n",
    "# plt.title('Sentiment Distribution of TSLA Tweets (Single Day)')\n",
    "# plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"sentiment_distribution_pie.png\", dpi=300)\n",
    "# plt.close()\n",
    "# print(\"\\nSentiment distribution pie chart saved as 'sentiment_distribution_pie.png'.\")\n",
    "\n",
    "# # Distribution of compound scores\n",
    "# plt.figure(figsize=(10,6))\n",
    "# sns.histplot(data['compound_score'], bins=20, kde=True, color='skyblue')\n",
    "# plt.title('Distribution of Compound Sentiment Scores')\n",
    "# plt.xlabel('Compound Score')\n",
    "# plt.ylabel('Number of Tweets')\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"compound_score_distribution.png\", dpi=300)\n",
    "# plt.close()\n",
    "# print(\"Compound score distribution histogram saved as 'compound_score_distribution.png'.\")\n",
    "\n",
    "# # Top 5 Positive Tweets\n",
    "# top_positive = data.sort_values(by='compound_score', ascending=False).head(5)\n",
    "# print(\"\\nTop 5 Positive Tweets:\")\n",
    "# print(top_positive[['tweet', 'compound_score']].to_string(index=False))\n",
    "\n",
    "# # Top 5 Negative Tweets\n",
    "# top_negative = data.sort_values(by='compound_score').head(5)\n",
    "# print(\"\\nTop 5 Negative Tweets:\")\n",
    "# print(top_negative[['tweet', 'compound_score']].to_string(index=False))\n",
    "\n",
    "# # =========================================\n",
    "# # 4. Temporal Analysis (If 'created_at' Available)\n",
    "# # =========================================\n",
    "\n",
    "# if has_created_at:\n",
    "#     # Check if 'created_at' has valid timestamps\n",
    "#     if data['created_at'].notnull().any():\n",
    "#         # Extract hour from 'created_at'\n",
    "#         data['hour'] = data['created_at'].dt.hour\n",
    "        \n",
    "#         # Aggregate average compound scores per hour\n",
    "#         hourly_sentiment = data.groupby('hour')['compound_score'].mean().reset_index()\n",
    "        \n",
    "#         # Line Plot of Hourly Sentiment\n",
    "#         plt.figure(figsize=(12,6))\n",
    "#         sns.lineplot(data=hourly_sentiment, x='hour', y='compound_score', marker='o', color='purple')\n",
    "#         plt.title('Hourly Average Compound Sentiment Scores')\n",
    "#         plt.xlabel('Hour of Day')\n",
    "#         plt.ylabel('Average Compound Sentiment Score')\n",
    "#         plt.xticks(range(0,24))\n",
    "#         plt.grid(True)\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(\"hourly_sentiment.png\", dpi=300)\n",
    "#         plt.close()\n",
    "#         print(\"Hourly sentiment line plot saved as 'hourly_sentiment.png'.\")\n",
    "#     else:\n",
    "#         print(\"No valid timestamps in 'created_at' to perform hourly analysis.\")\n",
    "# else:\n",
    "#     print(\"No 'created_at' column present. Skipping temporal analysis.\")\n",
    "\n",
    "# # =========================================\n",
    "# # 5. Content Analysis - Word Clouds\n",
    "# # =========================================\n",
    "\n",
    "# # Function to generate and save word clouds\n",
    "# def generate_wordcloud(text_array, title, filename):\n",
    "#     text_combined = ' '.join(text_array)\n",
    "#     wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_combined)\n",
    "#     plt.figure(figsize=(10,5))\n",
    "#     plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#     plt.title(title, fontsize=20)\n",
    "#     plt.axis('off')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(filename, dpi=300)\n",
    "#     plt.close()\n",
    "#     print(f\"Word cloud '{filename}' generated and saved.\")\n",
    "\n",
    "# # Generate word cloud for Positive Tweets\n",
    "# positive_tweets = data[data['sentiment_category'] == 'Positive']['cleaned_tweet'].values\n",
    "# if len(positive_tweets) > 0:\n",
    "#     generate_wordcloud(positive_tweets, 'Word Cloud of Positive Tweets', \"positive_wordcloud.png\")\n",
    "# else:\n",
    "#     print(\"No positive tweets available to generate a word cloud.\")\n",
    "\n",
    "# # Generate word cloud for Negative Tweets\n",
    "# negative_tweets = data[data['sentiment_category'] == 'Negative']['cleaned_tweet'].values\n",
    "# if len(negative_tweets) > 0:\n",
    "#     generate_wordcloud(negative_tweets, 'Word Cloud of Negative Tweets', \"negative_wordcloud.png\")\n",
    "# else:\n",
    "#     print(\"No negative tweets available to generate a word cloud.\")\n",
    "\n",
    "# # =========================================\n",
    "# # 6. Topic Modeling (LDA) on All Tweets\n",
    "# # =========================================\n",
    "\n",
    "# # Prepare data for LDA\n",
    "# all_tweets = data['cleaned_tweet'].values\n",
    "\n",
    "# # Vectorize the text data\n",
    "# vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "# dtm = vectorizer.fit_transform(all_tweets)\n",
    "\n",
    "# # Initialize and fit LDA model with 5 topics\n",
    "# lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "# lda.fit(dtm)\n",
    "\n",
    "# # Display the top words for each topic\n",
    "# print(\"\\nTopics Identified via LDA:\")\n",
    "# for index, topic in enumerate(lda.components_):\n",
    "#     top_features = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "#     print(f\"Topic #{index + 1}: {', '.join(top_features)}\")\n",
    "\n",
    "# # =========================================\n",
    "# # 7. Load and Inspect Stock Data\n",
    "# # =========================================\n",
    "\n",
    "# # Load stock data from 'TSLA_stock_data.csv'\n",
    "# try:\n",
    "#     stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "#     print(\"\\nTSLA_stock_data.csv loaded successfully.\")\n",
    "# except FileNotFoundError:\n",
    "#     raise FileNotFoundError(\"TSLA_stock_data.csv file not found in the current directory.\")\n",
    "\n",
    "# # Display the first few rows to verify\n",
    "# print(\"\\nSample Stock Data:\")\n",
    "# print(stock_data.head())\n",
    "\n",
    "# # Ensure necessary columns are present\n",
    "# required_stock_columns = ['Date', 'Close']\n",
    "# missing_stock_columns = [col for col in required_stock_columns if col not in stock_data.columns]\n",
    "# if missing_stock_columns:\n",
    "#     raise ValueError(f\"Missing columns in TSLA_stock_data.csv: {missing_stock_columns}\")\n",
    "\n",
    "# # Convert 'Date' to datetime\n",
    "# stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce')\n",
    "\n",
    "# # Ensure 'Close' is numeric\n",
    "# stock_data['Close'] = pd.to_numeric(stock_data['Close'], errors='coerce')\n",
    "\n",
    "# # Drop rows with missing 'Close' or 'Date'\n",
    "# stock_data.dropna(subset=['Close', 'Date'], inplace=True)\n",
    "\n",
    "# # Sort stock data by date\n",
    "# stock_data.sort_values('Date', inplace=True)\n",
    "\n",
    "# # Calculate daily returns\n",
    "# stock_data['Return'] = stock_data['Close'].pct_change()\n",
    "\n",
    "# # Display the last 5 rows of stock data\n",
    "# print(\"\\nProcessed Stock Data with Returns:\")\n",
    "# print(stock_data.tail())\n",
    "\n",
    "# # =========================================\n",
    "# # 8. Acknowledging Correlation Limitation\n",
    "# # =========================================\n",
    "\n",
    "# print(\"\\nNote: Correlation analysis between sentiment and stock performance is not feasible with only one day of sentiment data.\")\n",
    "# print(\"For correlation, multiple days of sentiment and corresponding stock returns are required.\")\n",
    "# print(\"However, you can still analyze and interpret the sentiment distribution and content for the single day.\")\n",
    "\n",
    "# # =========================================\n",
    "# # 9. Save the DataFrames (Optional)\n",
    "# # =========================================\n",
    "\n",
    "# # Uncomment the following lines if you wish to save the processed data for future use.\n",
    "\n",
    "# # data.to_csv('Processed_Tesla_Tweets.csv', index=False)\n",
    "# # stock_data.to_csv('Processed_TSLA_stock_data.csv', index=False)\n",
    "# # print(\"\\nProcessed data saved as 'Processed_Tesla_Tweets.csv' and 'Processed_TSLA_stock_data.csv'.\")\n",
    "\n",
    "# # =========================================\n",
    "# # 10. Summary of Outputs\n",
    "# # =========================================\n",
    "\n",
    "# print(\"\\nAnalysis Complete.\")\n",
    "# print(\"Generated the following visualizations:\")\n",
    "# print(\"- 'sentiment_distribution_pie.png'\")\n",
    "# print(\"- 'compound_score_distribution.png'\")\n",
    "# if has_created_at and 'hour' in data.columns:\n",
    "#     print(\"- 'hourly_sentiment.png'\")\n",
    "# if len(positive_tweets) > 0:\n",
    "#     print(\"- 'positive_wordcloud.png'\")\n",
    "# if len(negative_tweets) > 0:\n",
    "#     print(\"- 'negative_wordcloud.png'\")\n",
    "# print(\"\\nTop positive and negative tweets have been printed to the console.\")\n",
    "# print(\"Identified topics via LDA have been printed to the console.\")\n",
    "# print(\"Sample and processed stock data have been printed to the console.\")\n",
    "\n",
    "# =========================================\n",
    "# 0. Import Necessary Libraries\n",
    "# =========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Set plot styles for better aesthetics\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Download the VADER lexicon for sentiment analysis\n",
    "try:\n",
    "    nltk.download('vader_lexicon')\n",
    "except:\n",
    "    print(\"Could not download VADER lexicon. Please ensure it is available.\")\n",
    "\n",
    "# =========================================\n",
    "# 1. Load and Preprocess Tweet Data\n",
    "# =========================================\n",
    "\n",
    "# Load tweet data from 'Tesla.csv'\n",
    "try:\n",
    "    data = pd.read_csv('Tesla.csv')\n",
    "    print(\"Tesla.csv loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Tesla.csv file not found in the current directory.\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"\\nSample Tweet Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Ensure necessary columns are present\n",
    "required_columns = ['tweet', 'date']\n",
    "missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing columns in Tesla.csv: {missing_columns}\")\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "\n",
    "# Check for 'created_at' column for temporal analysis\n",
    "has_created_at = 'created_at' in data.columns\n",
    "\n",
    "if has_created_at:\n",
    "    try:\n",
    "        data['created_at'] = pd.to_datetime(data['created_at'], unit='ms', errors='coerce')\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting 'created_at': {e}\")\n",
    "\n",
    "# Preprocessing function for tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)      # Remove URLs\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)                         # Remove mentions\n",
    "    tweet = re.sub(r'#', '', tweet)                            # Remove hash symbol\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)                      # Remove special characters/punctuation\n",
    "    tweet = re.sub(r'\\d+', '', tweet)                          # Remove numbers\n",
    "    tweet = tweet.strip()                                      # Remove leading/trailing whitespace\n",
    "    return tweet\n",
    "\n",
    "# Apply preprocessing to tweets\n",
    "data['cleaned_tweet'] = data['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# =========================================\n",
    "# 2. Sentiment Analysis with VADER\n",
    "# =========================================\n",
    "\n",
    "# Initialize VADER sentiment analyzer\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores for each tweet\n",
    "sentiment_scores = data['cleaned_tweet'].apply(sentiment_analyzer.polarity_scores)\n",
    "\n",
    "# Extract individual sentiment scores into separate columns\n",
    "data['compound_score'] = sentiment_scores.apply(lambda x: x['compound'])\n",
    "data['positive_score'] = sentiment_scores.apply(lambda x: x['pos'])\n",
    "data['negative_score'] = sentiment_scores.apply(lambda x: x['neg'])\n",
    "data['neutral_score'] = sentiment_scores.apply(lambda x: x['neu'])\n",
    "\n",
    "# Categorize tweets based on compound scores\n",
    "def categorize_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "data['sentiment_category'] = data['compound_score'].apply(categorize_sentiment)\n",
    "\n",
    "# Display sentiment distribution\n",
    "print(\"\\nSentiment Category Counts:\")\n",
    "print(data['sentiment_category'].value_counts())\n",
    "\n",
    "# =========================================\n",
    "# 3. Descriptive Sentiment Analysis\n",
    "# =========================================\n",
    "\n",
    "# Overall sentiment distribution\n",
    "sentiment_counts = data['sentiment_category'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Pie Chart of Sentiment Distribution\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
    "        colors=['#66b3ff','#ff9999','#99ff99'], startangle=140)\n",
    "plt.title('Sentiment Distribution of TSLA Tweets (Single Day)')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"sentiment_distribution_pie.png\", dpi=300)\n",
    "plt.close()\n",
    "print(\"\\nSentiment distribution pie chart saved as 'sentiment_distribution_pie.png'.\")\n",
    "\n",
    "# Distribution of compound scores\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data['compound_score'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Compound Sentiment Scores')\n",
    "plt.xlabel('Compound Score')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"compound_score_distribution.png\", dpi=300)\n",
    "plt.close()\n",
    "print(\"Compound score distribution histogram saved as 'compound_score_distribution.png'.\")\n",
    "\n",
    "# Top 5 Positive Tweets\n",
    "top_positive = data.sort_values(by='compound_score', ascending=False).head(5)\n",
    "print(\"\\nTop 5 Positive Tweets:\")\n",
    "print(top_positive[['tweet', 'compound_score']].to_string(index=False))\n",
    "\n",
    "# Top 5 Negative Tweets\n",
    "top_negative = data.sort_values(by='compound_score').head(5)\n",
    "print(\"\\nTop 5 Negative Tweets:\")\n",
    "print(top_negative[['tweet', 'compound_score']].to_string(index=False))\n",
    "\n",
    "# =========================================\n",
    "# 4. Temporal Analysis (If 'created_at' Available)\n",
    "# =========================================\n",
    "\n",
    "if has_created_at:\n",
    "    # Check if 'created_at' has valid timestamps\n",
    "    if data['created_at'].notnull().any():\n",
    "        # Extract hour from 'created_at'\n",
    "        data['hour'] = data['created_at'].dt.hour\n",
    "        \n",
    "        # Aggregate average compound scores per hour\n",
    "        hourly_sentiment = data.groupby('hour')['compound_score'].mean().reset_index()\n",
    "        \n",
    "        # Line Plot of Hourly Sentiment\n",
    "        plt.figure(figsize=(12,6))\n",
    "        sns.lineplot(data=hourly_sentiment, x='hour', y='compound_score', marker='o', color='purple')\n",
    "        plt.title('Hourly Average Compound Sentiment Scores')\n",
    "        plt.xlabel('Hour of Day')\n",
    "        plt.ylabel('Average Compound Sentiment Score')\n",
    "        plt.xticks(range(0,24))\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"hourly_sentiment.png\", dpi=300)\n",
    "        plt.close()\n",
    "        print(\"Hourly sentiment line plot saved as 'hourly_sentiment.png'.\")\n",
    "    else:\n",
    "        print(\"No valid timestamps in 'created_at' to perform hourly analysis.\")\n",
    "else:\n",
    "    print(\"No 'created_at' column present. Skipping temporal analysis.\")\n",
    "\n",
    "# =========================================\n",
    "# 5. Content Analysis - Word Clouds\n",
    "# =========================================\n",
    "\n",
    "# Function to generate and save word clouds\n",
    "def generate_wordcloud(text_array, title, filename):\n",
    "    text_combined = ' '.join(text_array)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_combined)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Word cloud '{filename}' generated and saved.\")\n",
    "\n",
    "# Generate word cloud for Positive Tweets\n",
    "positive_tweets = data[data['sentiment_category'] == 'Positive']['cleaned_tweet'].values\n",
    "if len(positive_tweets) > 0:\n",
    "    generate_wordcloud(positive_tweets, 'Word Cloud of Positive Tweets', \"positive_wordcloud.png\")\n",
    "else:\n",
    "    print(\"No positive tweets available to generate a word cloud.\")\n",
    "\n",
    "# Generate word cloud for Negative Tweets\n",
    "negative_tweets = data[data['sentiment_category'] == 'Negative']['cleaned_tweet'].values\n",
    "if len(negative_tweets) > 0:\n",
    "    generate_wordcloud(negative_tweets, 'Word Cloud of Negative Tweets', \"negative_wordcloud.png\")\n",
    "else:\n",
    "    print(\"No negative tweets available to generate a word cloud.\")\n",
    "\n",
    "# =========================================\n",
    "# 6. Topic Modeling (LDA) on All Tweets\n",
    "# =========================================\n",
    "\n",
    "# Prepare data for LDA\n",
    "all_tweets = data['cleaned_tweet'].values\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = vectorizer.fit_transform(all_tweets)\n",
    "\n",
    "# Initialize and fit LDA model with 5 topics\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Display the top words for each topic\n",
    "print(\"\\nTopics Identified via LDA:\")\n",
    "for index, topic in enumerate(lda.components_):\n",
    "    top_features = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]\n",
    "    print(f\"Topic #{index + 1}: {', '.join(top_features)}\")\n",
    "\n",
    "# =========================================\n",
    "# 7. Load and Inspect Stock Data\n",
    "# =========================================\n",
    "\n",
    "# Load stock data from 'TSLA_stock_data.csv'\n",
    "try:\n",
    "    stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "    print(\"\\nTSLA_stock_data.csv loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"TSLA_stock_data.csv file not found in the current directory.\")\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(\"\\nSample Stock Data:\")\n",
    "print(stock_data.head())\n",
    "\n",
    "# Ensure necessary columns are present\n",
    "required_stock_columns = ['Date', 'Close']\n",
    "missing_stock_columns = [col for col in required_stock_columns if col not in stock_data.columns]\n",
    "if missing_stock_columns:\n",
    "    raise ValueError(f\"Missing columns in TSLA_stock_data.csv: {missing_stock_columns}\")\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "stock_data['Date'] = pd.to_datetime(stock_data['Date'], errors='coerce')\n",
    "\n",
    "# Ensure 'Close' is numeric\n",
    "stock_data['Close'] = pd.to_numeric(stock_data['Close'], errors='coerce')\n",
    "\n",
    "# Drop rows with missing 'Close' or 'Date'\n",
    "stock_data.dropna(subset=['Close', 'Date'], inplace=True)\n",
    "\n",
    "# Sort stock data by date\n",
    "stock_data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Calculate daily returns\n",
    "stock_data['Return'] = stock_data['Close'].pct_change()\n",
    "\n",
    "# Display the last 5 rows of stock data\n",
    "print(\"\\nProcessed Stock Data with Returns:\")\n",
    "print(stock_data.tail())\n",
    "\n",
    "# =========================================\n",
    "# 8. Acknowledging Correlation Limitation\n",
    "# =========================================\n",
    "\n",
    "print(\"\\nNote: Correlation analysis between sentiment and stock performance is not feasible with only one day of sentiment data.\")\n",
    "print(\"For correlation, multiple days of sentiment and corresponding stock returns are required.\")\n",
    "print(\"However, you can still analyze and interpret the sentiment distribution and content for the single day.\")\n",
    "\n",
    "# =========================================\n",
    "# 9. Save the DataFrames (Optional)\n",
    "# =========================================\n",
    "\n",
    "# Uncomment the following lines if you wish to save the processed data for future use.\n",
    "\n",
    "# data.to_csv('Processed_Tesla_Tweets.csv', index=False)\n",
    "# stock_data.to_csv('Processed_TSLA_stock_data.csv', index=False)\n",
    "# print(\"\\nProcessed data saved as 'Processed_Tesla_Tweets.csv' and 'Processed_TSLA_stock_data.csv'.\")\n",
    "\n",
    "# =========================================\n",
    "# 10. Summary of Outputs\n",
    "# =========================================\n",
    "\n",
    "print(\"\\nAnalysis Complete.\")\n",
    "print(\"Generated the following visualizations:\")\n",
    "print(\"- 'sentiment_distribution_pie.png'\")\n",
    "print(\"- 'compound_score_distribution.png'\")\n",
    "if has_created_at and 'hour' in data.columns:\n",
    "    print(\"- 'hourly_sentiment.png'\")\n",
    "if len(positive_tweets) > 0:\n",
    "    print(\"- 'positive_wordcloud.png'\")\n",
    "if len(negative_tweets) > 0:\n",
    "    print(\"- 'negative_wordcloud.png'\")\n",
    "print(\"\\nTop positive and negative tweets have been printed to the console.\")\n",
    "print(\"Identified topics via LDA have been printed to the console.\")\n",
    "print(\"Sample and processed stock data have been printed to the console.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
