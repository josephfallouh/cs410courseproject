{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tweets CSV...\n",
      "Tweets loaded. Shape: (10016, 39)\n",
      "\n",
      "============================================================\n",
      "Tweets Data After Timestamp Conversion\n",
      "------------------------------------------------------------\n",
      "Shape: (10016, 40)\n",
      "   Unnamed: 0                   id      conversation_id    created_at  \\\n",
      "0           0  1546541426317590528  1545826164564000768  1.657559e+12   \n",
      "1           1  1546541415857102850  1545826164564000768  1.657559e+12   \n",
      "2           2  1546541411897581568  1546541411897581568  1.657559e+12   \n",
      "3           3  1546541379110805508  1546340000500813824  1.657559e+12   \n",
      "4           4  1546541363470028800  1546541363470028800  1.657559e+12   \n",
      "\n",
      "                  date  timezone place  \\\n",
      "0  2022-07-11 17:06:24         0   NaN   \n",
      "1  2022-07-11 17:06:21         0   NaN   \n",
      "2  2022-07-11 17:06:20         0   NaN   \n",
      "3  2022-07-11 17:06:12         0   NaN   \n",
      "4  2022-07-11 17:06:09         0   NaN   \n",
      "\n",
      "                                               tweet language  \\\n",
      "0  @GailAlfarATX @elonmusk @Tesla @teslacn @Tesla...       en   \n",
      "1  @elonmusk @GailAlfarATX @Tesla @teslacn @Tesla...       en   \n",
      "2  @elonmusk #Think about buying a country , #Mex...       en   \n",
      "3  @get_innocuous Actual receipts, and yet you ha...       en   \n",
      "4  Tesla wall battery for the save! Power went ou...       en   \n",
      "\n",
      "                              hashtags  ... source  user_rt_id  user_rt  \\\n",
      "0                                   []  ...    NaN         NaN      NaN   \n",
      "1                                   []  ...    NaN         NaN      NaN   \n",
      "2  ['think', 'mexico', 'constitution']  ...    NaN         NaN      NaN   \n",
      "3                                   []  ...    NaN         NaN      NaN   \n",
      "4                                   []  ...    NaN         NaN      NaN   \n",
      "\n",
      "  retweet_id                                           reply_to  retweet_date  \\\n",
      "0        NaN  [{'screen_name': 'GailAlfarATX', 'name': 'Gail...           NaN   \n",
      "1        NaN  [{'screen_name': 'elonmusk', 'name': 'Elon Mus...           NaN   \n",
      "2        NaN                                                 []           NaN   \n",
      "3        NaN  [{'screen_name': 'get_innocuous', 'name': 'tra...           NaN   \n",
      "4        NaN                                                 []           NaN   \n",
      "\n",
      "  translate trans_src trans_dest                 timestamp  \n",
      "0       NaN       NaN        NaN 2022-07-11 17:06:24+00:00  \n",
      "1       NaN       NaN        NaN 2022-07-11 17:06:21+00:00  \n",
      "2       NaN       NaN        NaN 2022-07-11 17:06:20+00:00  \n",
      "3       NaN       NaN        NaN 2022-07-11 17:06:12+00:00  \n",
      "4       NaN       NaN        NaN 2022-07-11 17:06:09+00:00  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10016 entries, 0 to 10015\n",
      "Data columns (total 40 columns):\n",
      " #   Column           Non-Null Count  Dtype              \n",
      "---  ------           --------------  -----              \n",
      " 0   Unnamed: 0       10016 non-null  int64              \n",
      " 1   id               10016 non-null  int64              \n",
      " 2   conversation_id  10016 non-null  int64              \n",
      " 3   created_at       10016 non-null  float64            \n",
      " 4   date             10016 non-null  object             \n",
      " 5   timezone         10016 non-null  int64              \n",
      " 6   place            4 non-null      object             \n",
      " 7   tweet            10016 non-null  object             \n",
      " 8   language         10016 non-null  object             \n",
      " 9   hashtags         10016 non-null  object             \n",
      " 10  cashtags         10016 non-null  object             \n",
      " 11  user_id          10016 non-null  int64              \n",
      " 12  user_id_str      10016 non-null  int64              \n",
      " 13  username         10016 non-null  object             \n",
      " 14  name             10015 non-null  object             \n",
      " 15  day              10016 non-null  int64              \n",
      " 16  hour             10016 non-null  datetime64[ns, UTC]\n",
      " 17  link             10016 non-null  object             \n",
      " 18  urls             10016 non-null  object             \n",
      " 19  photos           10016 non-null  object             \n",
      " 20  video            10016 non-null  int64              \n",
      " 21  thumbnail        1215 non-null   object             \n",
      " 22  retweet          10016 non-null  bool               \n",
      " 23  nlikes           10016 non-null  int64              \n",
      " 24  nreplies         10016 non-null  int64              \n",
      " 25  nretweets        10016 non-null  int64              \n",
      " 26  quote_url        471 non-null    object             \n",
      " 27  search           10016 non-null  object             \n",
      " 28  near             0 non-null      float64            \n",
      " 29  geo              0 non-null      float64            \n",
      " 30  source           0 non-null      float64            \n",
      " 31  user_rt_id       0 non-null      float64            \n",
      " 32  user_rt          0 non-null      float64            \n",
      " 33  retweet_id       0 non-null      float64            \n",
      " 34  reply_to         10016 non-null  object             \n",
      " 35  retweet_date     0 non-null      float64            \n",
      " 36  translate        0 non-null      float64            \n",
      " 37  trans_src        0 non-null      float64            \n",
      " 38  trans_dest       0 non-null      float64            \n",
      " 39  timestamp        10016 non-null  datetime64[ns, UTC]\n",
      "dtypes: bool(1), datetime64[ns, UTC](2), float64(11), int64(11), object(15)\n",
      "memory usage: 3.0+ MB\n",
      "None\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Cleaned Data with Compound Scores\n",
      "------------------------------------------------------------\n",
      "Shape: (10016, 5)\n",
      "                                      original_tweet  \\\n",
      "0  @GailAlfarATX @elonmusk @Tesla @teslacn @Tesla...   \n",
      "1  @elonmusk @GailAlfarATX @Tesla @teslacn @Tesla...   \n",
      "2  @elonmusk #Think about buying a country , #Mex...   \n",
      "3  @get_innocuous Actual receipts, and yet you ha...   \n",
      "4  Tesla wall battery for the save! Power went ou...   \n",
      "\n",
      "                  timestamp                      hour  \\\n",
      "0 2022-07-11 17:06:24+00:00 2022-07-11 17:00:00+00:00   \n",
      "1 2022-07-11 17:06:21+00:00 2022-07-11 17:00:00+00:00   \n",
      "2 2022-07-11 17:06:20+00:00 2022-07-11 17:00:00+00:00   \n",
      "3 2022-07-11 17:06:12+00:00 2022-07-11 17:00:00+00:00   \n",
      "4 2022-07-11 17:06:09+00:00 2022-07-11 17:00:00+00:00   \n",
      "\n",
      "                                       cleaned_tweet  compound_score  \n",
      "0  i have six  of them still live at home being h...         -0.4767  \n",
      "1  then go for your dozen kids you are just missi...         -0.2960  \n",
      "2  think about buying a country  mexico  you coul...          0.5267  \n",
      "3  actual receipts and yet you havent asked anyon...          0.4404  \n",
      "4  tesla wall battery for the save power went out...          0.2732  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10016 entries, 0 to 10015\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype              \n",
      "---  ------          --------------  -----              \n",
      " 0   original_tweet  10016 non-null  object             \n",
      " 1   timestamp       10016 non-null  datetime64[ns, UTC]\n",
      " 2   hour            10016 non-null  datetime64[ns, UTC]\n",
      " 3   cleaned_tweet   10016 non-null  object             \n",
      " 4   compound_score  10016 non-null  float64            \n",
      "dtypes: datetime64[ns, UTC](2), float64(1), object(2)\n",
      "memory usage: 391.4+ KB\n",
      "None\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Hourly Sentiment Aggregation\n",
      "------------------------------------------------------------\n",
      "Shape: (14, 2)\n",
      "                       hour  average_compound\n",
      "0 2022-07-11 04:00:00+00:00          0.096378\n",
      "1 2022-07-11 05:00:00+00:00          0.032676\n",
      "2 2022-07-11 06:00:00+00:00          0.030375\n",
      "3 2022-07-11 07:00:00+00:00          0.030121\n",
      "4 2022-07-11 08:00:00+00:00          0.069604\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14 entries, 0 to 13\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   hour              14 non-null     datetime64[ns, UTC]\n",
      " 1   average_compound  14 non-null     float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(1)\n",
      "memory usage: 356.0 bytes\n",
      "None\n",
      "============================================================\n",
      "\n",
      "Loading Stock Data CSV...\n",
      "Stock data loaded. Shape: (5, 2)\n",
      "\n",
      "============================================================\n",
      "Stock Data After Renaming Columns\n",
      "------------------------------------------------------------\n",
      "Shape: (5, 2)\n",
      "    time   Close\n",
      "0  13:30  240.24\n",
      "1  14:30  236.85\n",
      "2  15:30  234.27\n",
      "3  16:30  237.53\n",
      "4  17:30  235.81\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   time    5 non-null      object \n",
      " 1   Close   5 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 212.0+ bytes\n",
      "None\n",
      "============================================================\n",
      "\n",
      "Common date extracted from tweets: 2022-07-11\n",
      "\n",
      "============================================================\n",
      "Stock Data After Timestamp & Hourly_Return Calculation\n",
      "------------------------------------------------------------\n",
      "Shape: (5, 5)\n",
      "    time   Close                 timestamp                      hour  \\\n",
      "0  13:30  240.24 2022-07-11 13:30:00+00:00 2022-07-11 13:00:00+00:00   \n",
      "1  14:30  236.85 2022-07-11 14:30:00+00:00 2022-07-11 14:00:00+00:00   \n",
      "2  15:30  234.27 2022-07-11 15:30:00+00:00 2022-07-11 15:00:00+00:00   \n",
      "3  16:30  237.53 2022-07-11 16:30:00+00:00 2022-07-11 16:00:00+00:00   \n",
      "4  17:30  235.81 2022-07-11 17:30:00+00:00 2022-07-11 17:00:00+00:00   \n",
      "\n",
      "   Hourly_Return  \n",
      "0            NaN  \n",
      "1      -0.014111  \n",
      "2      -0.010893  \n",
      "3       0.013916  \n",
      "4      -0.007241  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   time           5 non-null      object             \n",
      " 1   Close          5 non-null      float64            \n",
      " 2   timestamp      5 non-null      datetime64[ns, UTC]\n",
      " 3   hour           5 non-null      datetime64[ns, UTC]\n",
      " 4   Hourly_Return  4 non-null      float64            \n",
      "dtypes: datetime64[ns, UTC](2), float64(2), object(1)\n",
      "memory usage: 332.0+ bytes\n",
      "None\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "Merged Data\n",
      "------------------------------------------------------------\n",
      "Shape: (5, 6)\n",
      "                       hour  average_compound   time   Close  \\\n",
      "0 2022-07-11 13:00:00+00:00          0.054460  13:30  240.24   \n",
      "1 2022-07-11 14:00:00+00:00          0.053137  14:30  236.85   \n",
      "2 2022-07-11 15:00:00+00:00          0.083583  15:30  234.27   \n",
      "3 2022-07-11 16:00:00+00:00          0.081948  16:30  237.53   \n",
      "4 2022-07-11 17:00:00+00:00          0.087029  17:30  235.81   \n",
      "\n",
      "                  timestamp  Hourly_Return  \n",
      "0 2022-07-11 13:30:00+00:00            NaN  \n",
      "1 2022-07-11 14:30:00+00:00      -0.014111  \n",
      "2 2022-07-11 15:30:00+00:00      -0.010893  \n",
      "3 2022-07-11 16:30:00+00:00       0.013916  \n",
      "4 2022-07-11 17:30:00+00:00      -0.007241  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count  Dtype              \n",
      "---  ------            --------------  -----              \n",
      " 0   hour              5 non-null      datetime64[ns, UTC]\n",
      " 1   average_compound  5 non-null      float64            \n",
      " 2   time              5 non-null      object             \n",
      " 3   Close             5 non-null      float64            \n",
      " 4   timestamp         5 non-null      datetime64[ns, UTC]\n",
      " 5   Hourly_Return     4 non-null      float64            \n",
      "dtypes: datetime64[ns, UTC](2), float64(3), object(1)\n",
      "memory usage: 372.0+ bytes\n",
      "None\n",
      "============================================================\n",
      "\n",
      "Unique average_compound values: [0.05446006 0.0531371  0.08358308 0.08194848 0.08702933]\n",
      "Unique Hourly_Return values: [        nan -0.01411089 -0.01089297  0.01391557 -0.00724119]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 119\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m merged_data\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;129;01mand\u001b[39;00m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_compound\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly_Return\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    118\u001b[0m     spearman_corr, spearman_p \u001b[38;5;241m=\u001b[39m spearmanr(merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_compound\u001b[39m\u001b[38;5;124m'\u001b[39m], merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly_Return\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 119\u001b[0m     pearson_corr, pearson_p \u001b[38;5;241m=\u001b[39m pearsonr(merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_compound\u001b[39m\u001b[38;5;124m'\u001b[39m], merged_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly_Return\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpearman Correlation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspearman_corr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspearman_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPearson Correlation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpearson_corr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, p-value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpearson_p\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\folan\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:4838\u001b[0m, in \u001b[0;36mpearsonr\u001b[1;34m(x, y, alternative, method)\u001b[0m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;66;03m# Unlike np.linalg.norm or the expression sqrt((xm*xm).sum()),\u001b[39;00m\n\u001b[0;32m   4835\u001b[0m \u001b[38;5;66;03m# scipy.linalg.norm(xm) does not overflow if xm is, for example,\u001b[39;00m\n\u001b[0;32m   4836\u001b[0m \u001b[38;5;66;03m# [-5e210, 5e210, 3e200, -3e200]\u001b[39;00m\n\u001b[0;32m   4837\u001b[0m normxm \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mnorm(xm)\n\u001b[1;32m-> 4838\u001b[0m normym \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mnorm(ym)\n\u001b[0;32m   4840\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-13\u001b[39m\n\u001b[0;32m   4841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m normxm \u001b[38;5;241m<\u001b[39m threshold\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mabs\u001b[39m(xmean) \u001b[38;5;129;01mor\u001b[39;00m normym \u001b[38;5;241m<\u001b[39m threshold\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mabs\u001b[39m(ymean):\n\u001b[0;32m   4842\u001b[0m     \u001b[38;5;66;03m# If all the values in x (likewise y) are very close to the mean,\u001b[39;00m\n\u001b[0;32m   4843\u001b[0m     \u001b[38;5;66;03m# the loss of precision that occurs in the subtraction xm = x - xmean\u001b[39;00m\n\u001b[0;32m   4844\u001b[0m     \u001b[38;5;66;03m# might result in large errors in r.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\folan\\anaconda3\\Lib\\site-packages\\scipy\\linalg\\_misc.py:146\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(a, ord, axis, keepdims, check_finite)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# Differs from numpy only in non-finite handling and the use of blas.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_finite:\n\u001b[1;32m--> 146\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray_chkfinite(a)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     a \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(a)\n",
      "File \u001b[1;32mc:\\Users\\folan\\anaconda3\\Lib\\site-packages\\numpy\\lib\\function_base.py:630\u001b[0m, in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    628\u001b[0m a \u001b[38;5;241m=\u001b[39m asarray(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m typecodes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAllFloat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(a)\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray must not contain infs or NaNs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import Counter\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# If running for the first time, uncomment the following line:\n",
    "# nltk.download('vader_lexicon')\n",
    "\n",
    "# ------------------------\n",
    "# Debug Printing Function\n",
    "# ------------------------\n",
    "def debug_print(df, message, head=True):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(message)\n",
    "    print(\"-\"*60)\n",
    "    print(\"Shape:\", df.shape)\n",
    "    if head:\n",
    "        print(df.head())\n",
    "    print(df.info())\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# ------------------------\n",
    "# 1. Load and Preprocess Tweets\n",
    "# ------------------------\n",
    "print(\"Loading Tweets CSV...\")\n",
    "tweets_data = pd.read_csv('Tesla.csv')\n",
    "print(\"Tweets loaded. Shape:\", tweets_data.shape)\n",
    "\n",
    "# Convert 'date' to datetime with UTC timezone\n",
    "tweets_data['timestamp'] = pd.to_datetime(tweets_data['date'], utc=True, errors='coerce')\n",
    "tweets_data['hour'] = tweets_data['timestamp'].dt.floor('H')\n",
    "debug_print(tweets_data, \"Tweets Data After Timestamp Conversion\")\n",
    "\n",
    "# Create a new DataFrame to hold cleaned data and sentiment\n",
    "cleaned_data = pd.DataFrame()\n",
    "cleaned_data['original_tweet'] = tweets_data['tweet']\n",
    "cleaned_data['timestamp'] = tweets_data['timestamp']\n",
    "cleaned_data['hour'] = tweets_data['hour']\n",
    "cleaned_data['cleaned_tweet'] = \"\"\n",
    "cleaned_data['compound_score'] = 0.0\n",
    "\n",
    "# Preprocess tweets function\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)                     # Remove mentions\n",
    "    tweet = re.sub(r'#', '', tweet)                        # Remove hashtags\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)                  # Remove special characters\n",
    "    tweet = re.sub(r'\\d+', '', tweet)                      # Remove numbers\n",
    "    tweet = tweet.strip()                                  # Remove leading/trailing whitespace\n",
    "    return tweet.lower()\n",
    "\n",
    "cleaned_data['cleaned_tweet'] = cleaned_data['original_tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Sentiment analysis\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "cleaned_data['compound_score'] = cleaned_data['cleaned_tweet'].apply(lambda x: sentiment_analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "debug_print(cleaned_data, \"Cleaned Data with Compound Scores\")\n",
    "\n",
    "# Aggregate hourly sentiment scores\n",
    "hourly_sentiment = cleaned_data.groupby('hour')['compound_score'].mean().reset_index()\n",
    "hourly_sentiment.rename(columns={'compound_score': 'average_compound'}, inplace=True)\n",
    "debug_print(hourly_sentiment, \"Hourly Sentiment Aggregation\")\n",
    "\n",
    "# ------------------------\n",
    "# 2. Load and Process Stock Data\n",
    "# ------------------------\n",
    "print(\"Loading Stock Data CSV...\")\n",
    "stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "print(\"Stock data loaded. Shape:\", stock_data.shape)\n",
    "\n",
    "stock_data.rename(columns={'Time': 'time', 'Price': 'Close'}, inplace=True)\n",
    "debug_print(stock_data, \"Stock Data After Renaming Columns\")\n",
    "\n",
    "# Extract the common date from the tweets dataset (assuming all tweets are from the same date)\n",
    "if not tweets_data.empty:\n",
    "    common_date = pd.to_datetime(tweets_data['date'].iloc[0], errors='coerce').date()\n",
    "    print(\"Common date extracted from tweets:\", common_date)\n",
    "else:\n",
    "    print(\"Tweets data is empty, cannot extract common date.\")\n",
    "    common_date = pd.to_datetime('2022-07-11').date()  # fallback if empty\n",
    "\n",
    "# The times are in 24-hour format in TSLA_stock_data.csv now, so we can parse directly\n",
    "stock_data['timestamp'] = pd.to_datetime(\n",
    "    stock_data['time'].apply(lambda t: f\"{common_date} {t}\"),\n",
    "    format='%Y-%m-%d %H:%M',\n",
    "    utc=True,\n",
    "    errors='coerce'\n",
    ")\n",
    "stock_data['hour'] = stock_data['timestamp'].dt.floor('H')\n",
    "stock_data['Close'] = pd.to_numeric(stock_data['Close'], errors='coerce')\n",
    "stock_data['Hourly_Return'] = stock_data['Close'].pct_change()\n",
    "\n",
    "debug_print(stock_data, \"Stock Data After Timestamp & Hourly_Return Calculation\")\n",
    "\n",
    "# ------------------------\n",
    "# 3. Align Data\n",
    "# ------------------------\n",
    "merged_data = pd.merge(hourly_sentiment, stock_data, on='hour', how='inner')\n",
    "\n",
    "debug_print(merged_data, \"Merged Data\")\n",
    "\n",
    "# ------------------------\n",
    "# 4. Compute Spearman and Pearson Correlation\n",
    "# ------------------------\n",
    "# Check variation in average_compound and Hourly_Return\n",
    "print(\"Unique average_compound values:\", merged_data['average_compound'].unique())\n",
    "print(\"Unique Hourly_Return values:\", merged_data['Hourly_Return'].unique())\n",
    "\n",
    "if not merged_data.empty and merged_data['average_compound'].nunique() > 1 and merged_data['Hourly_Return'].nunique() > 1:\n",
    "    spearman_corr, spearman_p = spearmanr(merged_data['average_compound'], merged_data['Hourly_Return'])\n",
    "    pearson_corr, pearson_p = pearsonr(merged_data['average_compound'], merged_data['Hourly_Return'])\n",
    "    print(f\"Spearman Correlation: {spearman_corr}, p-value: {spearman_p}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr}, p-value: {pearson_p}\")\n",
    "else:\n",
    "    print(\"Insufficient or non-overlapping data for meaningful correlation.\")\n",
    "    # Print reasons why\n",
    "    if merged_data.empty:\n",
    "        print(\"Reason: merged_data is empty. No overlapping hours.\")\n",
    "    else:\n",
    "        if merged_data['average_compound'].nunique() <= 1:\n",
    "            print(\"Reason: average_compound has no variation.\")\n",
    "        if merged_data['Hourly_Return'].nunique() <= 1:\n",
    "            print(\"Reason: Hourly_Return has no variation.\")\n",
    "\n",
    "# ------------------------\n",
    "# 5. Visualizations\n",
    "# ------------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if not hourly_sentiment.empty:\n",
    "    # A) Line plot of average hourly sentiment over time\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(hourly_sentiment['hour'], hourly_sentiment['average_compound'], marker='o', color='blue')\n",
    "    plt.title('Average Hourly Sentiment (Compound Score)')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Average Compound Sentiment')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not stock_data.empty:\n",
    "    # B) Line plot of TSLA stock price over time\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(stock_data['timestamp'], stock_data['Close'], marker='o', color='green')\n",
    "    plt.title('TSLA Stock Price Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (Close)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not hourly_sentiment.empty and not stock_data.empty:\n",
    "    # C) Dual-axis line plot: Average Sentiment and Stock Close Price Over Time\n",
    "    fig, ax1 = plt.subplots(figsize=(12,6))\n",
    "\n",
    "    color = 'tab:blue'\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Avg Sentiment', color=color)\n",
    "    ax1.plot(hourly_sentiment['hour'], hourly_sentiment['average_compound'], marker='o', color=color, label='Avg Sentiment')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    color = 'tab:red'\n",
    "    # Need to align stock_data with hourly_sentiment time range\n",
    "    aligned_stock = stock_data[stock_data['hour'].isin(hourly_sentiment['hour'])]\n",
    "    ax2.set_ylabel('TSLA Close', color=color)\n",
    "    ax2.plot(aligned_stock['timestamp'], aligned_stock['Close'], marker='o', color=color, label='TSLA Close Price')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.title('Sentiment vs. Stock Price Over Time')\n",
    "    plt.show()\n",
    "\n",
    "if not cleaned_data.empty:\n",
    "    # D) Histogram of Compound Scores\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.histplot(cleaned_data['compound_score'], bins=20, kde=True, color='purple')\n",
    "    plt.title('Distribution of Compound Sentiment Scores')\n",
    "    plt.xlabel('Compound Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not merged_data.empty:\n",
    "    # E) Scatter plot comparing Average Compound vs Hourly Return\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.scatterplot(data=merged_data, x='average_compound', y='Hourly_Return', hue='Hourly_Return', palette='coolwarm', size='Hourly_Return', sizes=(50, 200))\n",
    "    plt.title('Average Sentiment vs. Hourly Return')\n",
    "    plt.xlabel('Average Compound Sentiment')\n",
    "    plt.ylabel('Hourly Return (%)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# F) Create a word cloud of the cleaned tweets\n",
    "if not cleaned_data.empty:\n",
    "    all_words = \" \".join(cleaned_data['cleaned_tweet'])\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords, max_words=200).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(15,7.5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Tweets', fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not cleaned_data.empty:\n",
    "    # G) Bar plot of most common words\n",
    "    word_list = all_words.split()\n",
    "    common_words = Counter(word_list).most_common(20)\n",
    "    common_words_df = pd.DataFrame(common_words, columns=['word', 'count'])\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.barplot(x='count', y='word', data=common_words_df, palette='Blues_r')\n",
    "    plt.title('Top 20 Most Common Words in Tweets')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Word')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not stock_data.empty:\n",
    "    # H) Line plot showing Hourly Return vs. Time\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(stock_data['timestamp'], stock_data['Hourly_Return']*100, marker='o', color='red')\n",
    "    plt.title('TSLA Hourly Return Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Hourly Return (%)')\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if not merged_data.empty:\n",
    "    # I) Correlation heatmap\n",
    "    plt.figure(figsize=(5,4))\n",
    "    corr = merged_data[['average_compound','Hourly_Return']].corr(method='spearman')\n",
    "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Spearman Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
