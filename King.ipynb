{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Function to get BERT sentiment\n",
    "def get_bert_sentiment(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return predictions[0].detach().numpy()\n",
    "\n",
    "print(\"Loading data...\")\n",
    "tweets_data = pd.read_csv('Tesla.csv')\n",
    "stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "\n",
    "# Basic tweet preprocessing\n",
    "print(\"\\nPreprocessing tweets...\")\n",
    "tweets_data['cleaned_tweet'] = tweets_data['tweet'].fillna('')\n",
    "tweets_data['cleaned_tweet'] = tweets_data['tweet'].apply(lambda x: str(x))\n",
    "tweets_data['cleaned_tweet'] = tweets_data['cleaned_tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x))\n",
    "tweets_data['cleaned_tweet'] = tweets_data['cleaned_tweet'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
    "tweets_data['cleaned_tweet'] = tweets_data['cleaned_tweet'].apply(lambda x: re.sub(r'#', '', x))\n",
    "tweets_data['created_at'] = pd.to_datetime(tweets_data['created_at'], unit='ms')\n",
    "tweets_data['hour'] = tweets_data['created_at'].dt.hour\n",
    "\n",
    "# VADER Analysis\n",
    "print(\"\\nPerforming VADER analysis...\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vader_scores = []\n",
    "\n",
    "for tweet in tweets_data['cleaned_tweet']:\n",
    "    scores = analyzer.polarity_scores(tweet)\n",
    "    vader_scores.append(scores)\n",
    "\n",
    "tweets_data['vader_compound'] = [score['compound'] for score in vader_scores]\n",
    "tweets_data['vader_positive'] = [score['pos'] for score in vader_scores]\n",
    "tweets_data['vader_negative'] = [score['neg'] for score in vader_scores]\n",
    "tweets_data['vader_neutral'] = [score['neu'] for score in vader_scores]\n",
    "\n",
    "# Display VADER results\n",
    "print(\"\\nVADER Analysis Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average Compound Score: {tweets_data['vader_compound'].mean():.3f}\")\n",
    "print(f\"Average Positive Score: {tweets_data['vader_positive'].mean():.3f}\")\n",
    "print(f\"Average Negative Score: {tweets_data['vader_negative'].mean():.3f}\")\n",
    "print(f\"Average Neutral Score: {tweets_data['vader_neutral'].mean():.3f}\")\n",
    "\n",
    "# VADER visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=tweets_data, x='vader_compound', bins=30)\n",
    "plt.title('Distribution of VADER Compound Scores')\n",
    "plt.xlabel('Compound Score')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('vader_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# BERT Analysis\n",
    "print(\"\\nLoading BERT model...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    \n",
    "    print(\"\\nPerforming BERT analysis...\")\n",
    "    bert_scores = []\n",
    "    batch_size = 8  # Smaller batch size for memory efficiency\n",
    "    \n",
    "    for i in range(0, len(tweets_data), batch_size):\n",
    "        batch_tweets = tweets_data['cleaned_tweet'][i:i+batch_size].tolist()\n",
    "        batch_scores = []\n",
    "        for tweet in batch_tweets:\n",
    "            sentiment_scores = get_bert_sentiment(tweet, tokenizer, model)\n",
    "            # Convert 1-5 scale to -1 to 1 scale for comparison with VADER\n",
    "            bert_score = (np.argmax(sentiment_scores) + 1 - 3) / 2\n",
    "            batch_scores.append(bert_score)\n",
    "        bert_scores.extend(batch_scores)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{len(tweets_data)} tweets\")\n",
    "    \n",
    "    tweets_data['bert_score'] = bert_scores\n",
    "    \n",
    "    # Display BERT results\n",
    "    print(\"\\nBERT Analysis Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Average BERT Score: {tweets_data['bert_score'].mean():.3f}\")\n",
    "    \n",
    "    # BERT visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=tweets_data, x='bert_score', bins=30)\n",
    "    plt.title('Distribution of BERT Sentiment Scores')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('bert_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Compare VADER and BERT\n",
    "    correlation = tweets_data['vader_compound'].corr(tweets_data['bert_score'])\n",
    "    print(f\"\\nCorrelation between VADER and BERT scores: {correlation:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during BERT analysis: {str(e)}\")\n",
    "    print(\"Continuing with VADER analysis only...\")\n",
    "\n",
    "# Stock Price Analysis\n",
    "print(\"\\nAnalyzing relationship with stock prices...\")\n",
    "stock_data['hour'] = stock_data['time'].str.split(':').str[0].astype(int)\n",
    "stock_data['Return'] = stock_data['price'].pct_change()\n",
    "\n",
    "# Aggregate sentiment by hour\n",
    "hourly_sentiment = tweets_data.groupby('hour').agg({\n",
    "    'vader_compound': 'mean',\n",
    "    'cleaned_tweet': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "merged_data = pd.merge(hourly_sentiment, stock_data, on='hour', how='inner')\n",
    "\n",
    "# Calculate correlation with stock returns\n",
    "vader_corr, vader_p = spearmanr(merged_data['vader_compound'], merged_data['Return'])\n",
    "\n",
    "print(\"\\nStock Return Correlations:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"VADER Correlation: {vader_corr:.3f} (p-value: {vader_p:.3f})\")\n",
    "\n",
    "# Display most extreme examples\n",
    "print(\"\\nMost Extreme Examples (VADER):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nMost Positive Tweet:\")\n",
    "print(tweets_data.loc[tweets_data['vader_compound'].idxmax(), 'cleaned_tweet'])\n",
    "print(f\"VADER score: {tweets_data['vader_compound'].max():.3f}\")\n",
    "\n",
    "print(\"\\nMost Negative Tweet:\")\n",
    "print(tweets_data.loc[tweets_data['vader_compound'].idxmin(), 'cleaned_tweet'])\n",
    "print(f\"VADER score: {tweets_data['vader_compound'].min():.3f}\")\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving results...\")\n",
    "tweets_data.to_csv('sentiment_analysis_results.csv', index=False)\n",
    "\n",
    "print(\"\\nAnalysis complete! Results have been saved to CSV and visualizations to PNG files.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
