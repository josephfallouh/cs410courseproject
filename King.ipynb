{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\folan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Preprocessing tweets...\n",
      "\n",
      "Performing VADER analysis...\n",
      "\n",
      "VADER Analysis Results:\n",
      "--------------------------------------------------\n",
      "Average Compound Score: 0.061\n",
      "Average Positive Score: 0.088\n",
      "Average Negative Score: 0.054\n",
      "Average Neutral Score: 0.844\n",
      "\n",
      "Loading BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\folan\\anaconda3\\Lib\\site-packages\\seaborn\\_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error during BERT analysis: \n",
      "AutoModelForSequenceClassification requires the PyTorch library but it was not found in your environment.\n",
      "However, we were able to find a TensorFlow installation. TensorFlow classes begin\n",
      "with \"TF\", but are otherwise identically named to our PyTorch classes. This\n",
      "means that the TF equivalent of the class you tried to import would be \"TFAutoModelForSequenceClassification\".\n",
      "If you want to use TensorFlow, please use TF classes instead!\n",
      "\n",
      "If you really do want to use PyTorch please go to\n",
      "https://pytorch.org/get-started/locally/ and follow the instructions that\n",
      "match your environment.\n",
      "\n",
      "Continuing with VADER analysis only...\n",
      "\n",
      "Analyzing relationship with stock prices...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\folan\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# Stock Price Analysis\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing relationship with stock prices...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 113\u001b[0m stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mstr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    114\u001b[0m stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReturn\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m stock_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mpct_change()\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Aggregate sentiment by hour\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\folan\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\folan\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'time'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Function to get BERT sentiment\n",
    "def get_bert_sentiment(text, tokenizer, model):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    return predictions[0].detach().numpy()\n",
    "\n",
    "print(\"Loading data...\")\n",
    "tweets_data = pd.read_csv('Tesla.csv')\n",
    "stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "\n",
    "# Basic tweet preprocessing\n",
    "print(\"\\nPreprocessing tweets...\")\n",
    "tweets_data['cleaned_tweet'] = tweets_data['tweet'].fillna('')\n",
    "tweets_data['cleaned_tweet'] = tweets_data['tweet'].apply(lambda x: str(x))\n",
    "tweets_data['cleaned_tweet'] = tweets_data['cleaned_tweet'].apply(lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x))\n",
    "tweets_data['cleaned_tweet'] = tweets_data['cleaned_tweet'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
    "tweets_data['cleaned_tweet'] = tweets_data['cleaned_tweet'].apply(lambda x: re.sub(r'#', '', x))\n",
    "tweets_data['created_at'] = pd.to_datetime(tweets_data['created_at'], unit='ms')\n",
    "tweets_data['hour'] = tweets_data['created_at'].dt.hour\n",
    "\n",
    "# VADER Analysis\n",
    "print(\"\\nPerforming VADER analysis...\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vader_scores = []\n",
    "\n",
    "for tweet in tweets_data['cleaned_tweet']:\n",
    "    scores = analyzer.polarity_scores(tweet)\n",
    "    vader_scores.append(scores)\n",
    "\n",
    "tweets_data['vader_compound'] = [score['compound'] for score in vader_scores]\n",
    "tweets_data['vader_positive'] = [score['pos'] for score in vader_scores]\n",
    "tweets_data['vader_negative'] = [score['neg'] for score in vader_scores]\n",
    "tweets_data['vader_neutral'] = [score['neu'] for score in vader_scores]\n",
    "\n",
    "# Display VADER results\n",
    "print(\"\\nVADER Analysis Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average Compound Score: {tweets_data['vader_compound'].mean():.3f}\")\n",
    "print(f\"Average Positive Score: {tweets_data['vader_positive'].mean():.3f}\")\n",
    "print(f\"Average Negative Score: {tweets_data['vader_negative'].mean():.3f}\")\n",
    "print(f\"Average Neutral Score: {tweets_data['vader_neutral'].mean():.3f}\")\n",
    "\n",
    "# VADER visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=tweets_data, x='vader_compound', bins=30)\n",
    "plt.title('Distribution of VADER Compound Scores')\n",
    "plt.xlabel('Compound Score')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('vader_distribution.png')\n",
    "plt.close()\n",
    "\n",
    "# BERT Analysis\n",
    "print(\"\\nLoading BERT model...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "    \n",
    "    print(\"\\nPerforming BERT analysis...\")\n",
    "    bert_scores = []\n",
    "    batch_size = 8  # Smaller batch size for memory efficiency\n",
    "    \n",
    "    for i in range(0, len(tweets_data), batch_size):\n",
    "        batch_tweets = tweets_data['cleaned_tweet'][i:i+batch_size].tolist()\n",
    "        batch_scores = []\n",
    "        for tweet in batch_tweets:\n",
    "            sentiment_scores = get_bert_sentiment(tweet, tokenizer, model)\n",
    "            # Convert 1-5 scale to -1 to 1 scale for comparison with VADER\n",
    "            bert_score = (np.argmax(sentiment_scores) + 1 - 3) / 2\n",
    "            batch_scores.append(bert_score)\n",
    "        bert_scores.extend(batch_scores)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processed {i}/{len(tweets_data)} tweets\")\n",
    "    \n",
    "    tweets_data['bert_score'] = bert_scores\n",
    "    \n",
    "    # Display BERT results\n",
    "    print(\"\\nBERT Analysis Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Average BERT Score: {tweets_data['bert_score'].mean():.3f}\")\n",
    "    \n",
    "    # BERT visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=tweets_data, x='bert_score', bins=30)\n",
    "    plt.title('Distribution of BERT Sentiment Scores')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('bert_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Compare VADER and BERT\n",
    "    correlation = tweets_data['vader_compound'].corr(tweets_data['bert_score'])\n",
    "    print(f\"\\nCorrelation between VADER and BERT scores: {correlation:.3f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError during BERT analysis: {str(e)}\")\n",
    "    print(\"Continuing with VADER analysis only...\")\n",
    "\n",
    "# Stock Price Analysis\n",
    "print(\"\\nAnalyzing relationship with stock prices...\")\n",
    "stock_data['hour'] = stock_data['time'].str.split(':').str[0].astype(int)\n",
    "stock_data['Return'] = stock_data['price'].pct_change()\n",
    "\n",
    "# Aggregate sentiment by hour\n",
    "hourly_sentiment = tweets_data.groupby('hour').agg({\n",
    "    'vader_compound': 'mean',\n",
    "    'cleaned_tweet': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "merged_data = pd.merge(hourly_sentiment, stock_data, on='hour', how='inner')\n",
    "\n",
    "# Calculate correlation with stock returns\n",
    "vader_corr, vader_p = spearmanr(merged_data['vader_compound'], merged_data['Return'])\n",
    "\n",
    "print(\"\\nStock Return Correlations:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"VADER Correlation: {vader_corr:.3f} (p-value: {vader_p:.3f})\")\n",
    "\n",
    "# Display most extreme examples\n",
    "print(\"\\nMost Extreme Examples (VADER):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\\nMost Positive Tweet:\")\n",
    "print(tweets_data.loc[tweets_data['vader_compound'].idxmax(), 'cleaned_tweet'])\n",
    "print(f\"VADER score: {tweets_data['vader_compound'].max():.3f}\")\n",
    "\n",
    "print(\"\\nMost Negative Tweet:\")\n",
    "print(tweets_data.loc[tweets_data['vader_compound'].idxmin(), 'cleaned_tweet'])\n",
    "print(f\"VADER score: {tweets_data['vader_compound'].min():.3f}\")\n",
    "\n",
    "# Save results\n",
    "print(\"\\nSaving results...\")\n",
    "tweets_data.to_csv('sentiment_analysis_results.csv', index=False)\n",
    "\n",
    "print(\"\\nAnalysis complete! Results have been saved to CSV and visualizations to PNG files.\")\n",
    "\n",
    "# //////////////////////////////////////\n",
    "# Spearman Correlation\n",
    "\n",
    "# Stock Price Analysis\n",
    "print(\"\\nAnalyzing relationship with stock prices...\")\n",
    "\n",
    "# Prepare stock data\n",
    "stock_data['hour'] = stock_data['time'].str.split(':').str[0].astype(int)\n",
    "stock_hours = sorted(stock_data['hour'].unique())\n",
    "print(\"\\nStock data hours available:\", stock_hours)\n",
    "\n",
    "# Calculate returns\n",
    "stock_data['Return'] = stock_data['price'].pct_change()\n",
    "\n",
    "# Aggregate sentiment by hour\n",
    "hourly_sentiment = tweets_data.groupby('hour').agg({\n",
    "    'vader_compound': 'mean',\n",
    "    'vader_positive': 'mean',\n",
    "    'vader_negative': 'mean',\n",
    "    'cleaned_tweet': 'count'\n",
    "}).reset_index()\n",
    "\n",
    "sentiment_hours = sorted(hourly_sentiment['hour'].unique())\n",
    "print(\"Sentiment data hours available:\", sentiment_hours)\n",
    "\n",
    "# Find overlapping hours\n",
    "overlapping_hours = sorted(set(stock_hours) & set(sentiment_hours))\n",
    "print(\"\\nOverlapping hours (13-17):\", overlapping_hours)\n",
    "\n",
    "# Filter data for overlapping hours only\n",
    "stock_data_filtered = stock_data[stock_data['hour'].isin(overlapping_hours)]\n",
    "sentiment_data_filtered = hourly_sentiment[hourly_sentiment['hour'].isin(overlapping_hours)]\n",
    "\n",
    "# Merge hourly sentiment with stock data\n",
    "merged_data = pd.merge(stock_data_filtered, sentiment_data_filtered, on='hour', how='inner')\n",
    "\n",
    "print(\"\\nHourly Analysis (13:00 - 17:00):\")\n",
    "print(\"-\" * 65)\n",
    "print(\"Hour | Price | Return  | Sentiment | Tweet Count\")\n",
    "print(\"-\" * 65)\n",
    "for _, row in merged_data.iterrows():\n",
    "    print(f\"{row['hour']:02d}:00 | {row['price']:6.2f} | {row['Return']:7.3%} | {row['vader_compound']:9.3f} | {int(row['cleaned_tweet']):11d}\")\n",
    "\n",
    "# Calculate correlations\n",
    "if len(merged_data) > 1:\n",
    "    vader_corr, vader_p = spearmanr(merged_data['vader_compound'], \n",
    "                                   merged_data['Return'], \n",
    "                                   nan_policy='omit')\n",
    "    \n",
    "    print(\"\\nStock Return Correlations (13:00 - 17:00):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"VADER Compound Score vs Returns:\")\n",
    "    print(f\"  Correlation: {vader_corr:.3f}\")\n",
    "    print(f\"  p-value: {vader_p:.3f}\")\n",
    "    print(f\"  Number of hours analyzed: {len(merged_data)}\")\n",
    "    \n",
    "    # Additional correlations\n",
    "    pos_corr, pos_p = spearmanr(merged_data['vader_positive'], merged_data['Return'])\n",
    "    neg_corr, neg_p = spearmanr(merged_data['vader_negative'], merged_data['Return'])\n",
    "    \n",
    "    print(\"\\nAdditional correlations:\")\n",
    "    print(f\"Positive sentiment vs Returns: {pos_corr:.3f} (p-value: {pos_p:.3f})\")\n",
    "    print(f\"Negative sentiment vs Returns: {neg_corr:.3f} (p-value: {neg_p:.3f})\")\n",
    "else:\n",
    "    print(\"\\nNot enough overlapping data points for correlation analysis\")\n",
    "\n",
    "# Visualize the relationship\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "# Plot 1: Price and Sentiment\n",
    "ax1 = plt.gca()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot stock price\n",
    "line1 = ax1.plot(merged_data['hour'], merged_data['price'], \n",
    "                 color='blue', label='Stock Price')\n",
    "ax1.set_xlabel('Hour')\n",
    "ax1.set_ylabel('Stock Price', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "# Plot sentiment\n",
    "line2 = ax2.plot(merged_data['hour'], merged_data['vader_compound'], \n",
    "                 color='red', linestyle='--', label='Sentiment')\n",
    "ax2.set_ylabel('Sentiment Score', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "# Add legend\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc='upper left')\n",
    "\n",
    "plt.title('Stock Price and Sentiment During Trading Hours')\n",
    "\n",
    "# Plot 2: Scatter plot of Returns vs Sentiment\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(merged_data['vader_compound'], merged_data['Return'])\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Stock Return')\n",
    "plt.title('Returns vs Sentiment Correlation')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(merged_data['vader_compound'], merged_data['Return'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(merged_data['vader_compound'], \n",
    "         p(merged_data['vader_compound']), \n",
    "         \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trading_hours_analysis.png')\n",
    "plt.close()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary Statistics (13:00 - 17:00):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Average sentiment: {merged_data['vader_compound'].mean():.3f}\")\n",
    "print(f\"Average stock return: {merged_data['Return'].mean():.3%}\")\n",
    "print(f\"Total tweets analyzed: {merged_data['cleaned_tweet'].sum()}\")\n",
    "print(f\"Average tweets per hour: {merged_data['cleaned_tweet'].mean():.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
