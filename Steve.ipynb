{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\folan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spearman Correlation: 0.39999999999999997, p-value: 0.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "from scipy.stats import spearmanr\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# ------------------------\n",
    "# 1. Load and Preprocess Tweets\n",
    "# ------------------------\n",
    "tweets_data = pd.read_csv('Tesla.csv')\n",
    "tweets_data['timestamp'] = pd.to_datetime(tweets_data['date'])\n",
    "tweets_data['hour'] = tweets_data['timestamp'].dt.floor('H')\n",
    "\n",
    "cleaned_data = pd.DataFrame()\n",
    "cleaned_data['original_tweet'] = tweets_data['tweet']\n",
    "cleaned_data['timestamp'] = tweets_data['timestamp']\n",
    "cleaned_data['hour'] = tweets_data['hour']\n",
    "cleaned_data['cleaned_tweet'] = \"\"\n",
    "cleaned_data['compound_score'] = 0.0\n",
    "\n",
    "# Preprocess tweets\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = str(tweet)\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\S+', '', tweet)  # Remove URLs\n",
    "    tweet = re.sub(r'@\\w+', '', tweet)  # Remove mentions\n",
    "    tweet = re.sub(r'#', '', tweet)  # Remove hashtags\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)  # Remove special characters\n",
    "    tweet = re.sub(r'\\d+', '', tweet)  # Remove numbers\n",
    "    tweet = tweet.strip()  # Remove leading/trailing whitespace\n",
    "    return tweet\n",
    "\n",
    "cleaned_data['cleaned_tweet'] = tweets_data['tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Sentiment analysis\n",
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "cleaned_data['compound_score'] = cleaned_data['cleaned_tweet'].apply(lambda x: sentiment_analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Aggregate hourly sentiment scores\n",
    "hourly_sentiment = cleaned_data.groupby('hour')['compound_score'].mean().reset_index()\n",
    "hourly_sentiment.rename(columns={'compound_score': 'average_compound'}, inplace=True)\n",
    "\n",
    "# ------------------------\n",
    "# 2. Enhance Stock Data with Full Timestamps\n",
    "# ------------------------\n",
    "stock_data = pd.read_csv('TSLA_stock_data.csv')\n",
    "stock_data.rename(columns={'Time': 'time', 'Price': 'Close'}, inplace=True)\n",
    "\n",
    "# Extract the common date from the tweets dataset\n",
    "common_date = pd.to_datetime(tweets_data['date'].iloc[0]).date()\n",
    "\n",
    "# Combine common date with stock times to create full timestamps\n",
    "stock_data['timestamp'] = pd.to_datetime(str(common_date) + ' ' + stock_data['time'])\n",
    "stock_data['hour'] = stock_data['timestamp'].dt.floor('H')\n",
    "\n",
    "# Calculate hourly returns\n",
    "stock_data['Close'] = pd.to_numeric(stock_data['Close'], errors='coerce')\n",
    "stock_data['Hourly_Return'] = stock_data['Close'].pct_change()\n",
    "\n",
    "# ------------------------\n",
    "# 3. Align Data\n",
    "# ------------------------\n",
    "merged_data = pd.merge(hourly_sentiment, stock_data, on='hour', how='inner')\n",
    "\n",
    "# ------------------------\n",
    "# 4. Compute Spearman Correlation\n",
    "# ------------------------\n",
    "if not merged_data.empty and merged_data['average_compound'].nunique() > 1 and merged_data['Hourly_Return'].nunique() > 1:\n",
    "    correlation, p_value = spearmanr(merged_data['average_compound'], merged_data['Hourly_Return'])\n",
    "    print(f\"Spearman Correlation: {correlation}, p-value: {p_value}\")\n",
    "else:\n",
    "    print(\"Insufficient or non-overlapping data for meaningful correlation.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
